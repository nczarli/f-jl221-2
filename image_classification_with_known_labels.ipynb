{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model_checkpoint = \"microsoft/swin-tiny-patch4-window7-224\" # pre-trained model from which to fine-tune\n",
    "batch_size = 32 # batch size for training and evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "37223a2f368348899aa7fe8aad76700e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/9044 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default-250cab40a46cd884\n",
      "Found cached dataset imagefolder (C:/Users/Nikodem/.cache/huggingface/datasets/imagefolder/default-250cab40a46cd884/0.0.0/37fbb85cc714a338bea574ac6c7d0b5be5aff46c1862c1989b20e0771199e93f)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0048d1e1a1984e6bafe80fecca8c30ca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at C:\\Users\\Nikodem\\.cache\\huggingface\\datasets\\imagefolder\\default-250cab40a46cd884\\0.0.0\\37fbb85cc714a338bea574ac6c7d0b5be5aff46c1862c1989b20e0771199e93f\\cache-ff668379e892e942.arrow\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "dataset = load_dataset(\"imagefolder\", data_dir='extracted_cherry_images', task='image-classification')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Nikodem\\AppData\\Local\\Temp\\ipykernel_16420\\1780215247.py:3: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library ðŸ¤— Evaluate: https://huggingface.co/docs/evaluate\n",
      "  metric = load_metric(\"accuracy\")\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_metric\n",
    "\n",
    "metric = load_metric(\"accuracy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['image', 'labels'],\n",
       "        num_rows: 9044\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'image': <PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=100x100>,\n",
       " 'labels': 0}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example = dataset[\"train\"][10]\n",
    "example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'image': Image(decode=True, id=None),\n",
       " 'labels': ClassLabel(names=['pits', 'pitted'], id=None)}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[\"train\"].features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=100x100>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example['image']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<PIL.Image.Image image mode=RGB size=200x200>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example['image'].resize((200, 200))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example['labels']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ClassLabel(names=['pits', 'pitted'], id=None)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[\"train\"].features[\"labels\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'pitted'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "labels = dataset[\"train\"].features[\"labels\"].names\n",
    "label2id, id2label = dict(), dict()\n",
    "for i, label in enumerate(labels):\n",
    "    label2id[label] = i\n",
    "    id2label[i] = label\n",
    "\n",
    "id2label[1]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ViTImageProcessor {\n",
       "  \"do_normalize\": true,\n",
       "  \"do_rescale\": true,\n",
       "  \"do_resize\": true,\n",
       "  \"feature_extractor_type\": \"ViTFeatureExtractor\",\n",
       "  \"image_mean\": [\n",
       "    0.485,\n",
       "    0.456,\n",
       "    0.406\n",
       "  ],\n",
       "  \"image_processor_type\": \"ViTImageProcessor\",\n",
       "  \"image_std\": [\n",
       "    0.229,\n",
       "    0.224,\n",
       "    0.225\n",
       "  ],\n",
       "  \"resample\": 3,\n",
       "  \"rescale_factor\": 0.00392156862745098,\n",
       "  \"size\": {\n",
       "    \"height\": 224,\n",
       "    \"width\": 224\n",
       "  }\n",
       "}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoFeatureExtractor\n",
    "\n",
    "feature_extractor = AutoFeatureExtractor.from_pretrained(model_checkpoint)\n",
    "feature_extractor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.transforms import (\n",
    "    CenterCrop,\n",
    "    Compose,\n",
    "    Normalize,\n",
    "    RandomHorizontalFlip,\n",
    "    RandomResizedCrop,\n",
    "    Resize,\n",
    "    ToTensor,\n",
    ")\n",
    "\n",
    "normalize = Normalize(mean=feature_extractor.image_mean, std=feature_extractor.image_std)\n",
    "train_transforms = Compose(\n",
    "        [\n",
    "            RandomResizedCrop((feature_extractor.size[\"height\"], feature_extractor.size[\"width\"])),\n",
    "            RandomHorizontalFlip(),\n",
    "            ToTensor(),\n",
    "            normalize,\n",
    "        ]\n",
    "    )\n",
    "\n",
    "val_transforms = Compose(\n",
    "        [\n",
    "            Resize((feature_extractor.size[\"height\"], feature_extractor.size[\"width\"])),\n",
    "            CenterCrop((feature_extractor.size[\"height\"], feature_extractor.size[\"width\"])),\n",
    "            ToTensor(),\n",
    "            normalize,\n",
    "        ]\n",
    "    )\n",
    "\n",
    "def preprocess_train(example_batch):\n",
    "    \"\"\"Apply train_transforms across a batch.\"\"\"\n",
    "    example_batch[\"pixel_values\"] = [\n",
    "        train_transforms(image.convert(\"RGB\")) for image in example_batch[\"image\"]\n",
    "    ]\n",
    "    return example_batch\n",
    "\n",
    "def preprocess_val(example_batch):\n",
    "    \"\"\"Apply val_transforms across a batch.\"\"\"\n",
    "    example_batch[\"pixel_values\"] = [val_transforms(image.convert(\"RGB\")) for image in example_batch[\"image\"]]\n",
    "    return example_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split up training into training + validation\n",
    "splits = dataset[\"train\"].train_test_split(test_size=0.1)\n",
    "train_ds = splits['train']\n",
    "val_ds = splits['test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'image': <PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=100x100>,\n",
       " 'labels': 1}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_ds[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds.set_transform(preprocess_train)\n",
    "val_ds.set_transform(preprocess_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'image': <PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=100x100>,\n",
       " 'labels': 1,\n",
       " 'pixel_values': tensor([[[2.2489, 2.2489, 2.2489,  ..., 2.2489, 2.2489, 2.2489],\n",
       "          [2.2489, 2.2489, 2.2489,  ..., 2.2489, 2.2489, 2.2489],\n",
       "          [2.2489, 2.2489, 2.2489,  ..., 2.2489, 2.2489, 2.2489],\n",
       "          ...,\n",
       "          [2.2489, 2.2489, 2.2489,  ..., 2.2489, 2.2489, 2.2489],\n",
       "          [2.2489, 2.2489, 2.2489,  ..., 2.2489, 2.2489, 2.2489],\n",
       "          [2.2489, 2.2489, 2.2489,  ..., 2.2489, 2.2489, 2.2489]],\n",
       " \n",
       "         [[2.4286, 2.4286, 2.4286,  ..., 2.4286, 2.4286, 2.4286],\n",
       "          [2.4286, 2.4286, 2.4286,  ..., 2.4286, 2.4286, 2.4286],\n",
       "          [2.4286, 2.4286, 2.4286,  ..., 2.4286, 2.4286, 2.4286],\n",
       "          ...,\n",
       "          [2.4286, 2.4286, 2.4286,  ..., 2.4286, 2.4286, 2.4286],\n",
       "          [2.4286, 2.4286, 2.4286,  ..., 2.4286, 2.4286, 2.4286],\n",
       "          [2.4286, 2.4286, 2.4286,  ..., 2.4286, 2.4286, 2.4286]],\n",
       " \n",
       "         [[2.6400, 2.6400, 2.6400,  ..., 2.6400, 2.6400, 2.6400],\n",
       "          [2.6400, 2.6400, 2.6400,  ..., 2.6400, 2.6400, 2.6400],\n",
       "          [2.6400, 2.6400, 2.6400,  ..., 2.6400, 2.6400, 2.6400],\n",
       "          ...,\n",
       "          [2.6400, 2.6400, 2.6400,  ..., 2.6400, 2.6400, 2.6400],\n",
       "          [2.6400, 2.6400, 2.6400,  ..., 2.6400, 2.6400, 2.6400],\n",
       "          [2.6400, 2.6400, 2.6400,  ..., 2.6400, 2.6400, 2.6400]]])}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_ds[0]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of SwinForImageClassification were not initialized from the model checkpoint at microsoft/swin-tiny-patch4-window7-224 and are newly initialized because the shapes did not match:\n",
      "- classifier.weight: found shape torch.Size([1000, 768]) in the checkpoint and torch.Size([2, 768]) in the model instantiated\n",
      "- classifier.bias: found shape torch.Size([1000]) in the checkpoint and torch.Size([2]) in the model instantiated\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from transformers import AutoModelForImageClassification, TrainingArguments, Trainer\n",
    "\n",
    "model = AutoModelForImageClassification.from_pretrained(\n",
    "    model_checkpoint, \n",
    "    label2id=label2id,\n",
    "    id2label=id2label,\n",
    "    ignore_mismatched_sizes = True, # provide this in case you're planning to fine-tune an already fine-tuned checkpoint\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model_name = model_checkpoint.split(\"/\")[-1]\n",
    "\n",
    "args = TrainingArguments(\n",
    "    f\"{model_name}-finetuned-eurosat\",\n",
    "    remove_unused_columns=False,\n",
    "    evaluation_strategy = \"epoch\",\n",
    "    save_strategy = \"epoch\",\n",
    "    learning_rate=5e-5,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    gradient_accumulation_steps=4,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    num_train_epochs=3,\n",
    "    warmup_ratio=0.1,\n",
    "    logging_steps=10,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"accuracy\",\n",
    "    push_to_hub=False,\n",
    ")\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "\n",
    "# the compute_metrics function takes a Named Tuple as input:\n",
    "# predictions, which are the logits of the model as Numpy arrays,\n",
    "# and label_ids, which are the ground-truth labels as Numpy arrays.\n",
    "def compute_metrics(eval_pred):\n",
    "    \"\"\"Computes accuracy on a batch of predictions\"\"\"\n",
    "    predictions = np.argmax(eval_pred.predictions, axis=1)\n",
    "    return metric.compute(predictions=predictions, references=eval_pred.label_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch\n",
    "\n",
    "def collate_fn(examples):\n",
    "    pixel_values = torch.stack([example[\"pixel_values\"] for example in examples])\n",
    "    labels = torch.tensor([example[\"label\"] for example in examples])\n",
    "    return {\"pixel_values\": pixel_values, \"labels\": labels}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "\n",
    "# the compute_metrics function takes a Named Tuple as input:\n",
    "# predictions, which are the logits of the model as Numpy arrays,\n",
    "# and label_ids, which are the ground-truth labels as Numpy arrays.\n",
    "def compute_metrics(eval_pred):\n",
    "    \"\"\"Computes accuracy on a batch of predictions\"\"\"\n",
    "    predictions = np.argmax(eval_pred.predictions, axis=1)\n",
    "    return metric.compute(predictions=predictions, references=eval_pred.label_ids)\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch\n",
    "\n",
    "def collate_fn(examples):\n",
    "    pixel_values = torch.stack([example[\"pixel_values\"] for example in examples])\n",
    "    labels = torch.tensor([example[\"labels\"] for example in examples])\n",
    "    return {\"pixel_values\": pixel_values, \"labels\": labels}\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a5cd0a6f10e54ee7915c4e03fcae7847",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.svâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "from huggingface_hub import notebook_login\n",
    "\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model,\n",
    "    args,\n",
    "    train_dataset=train_ds,\n",
    "    eval_dataset=val_ds,\n",
    "    tokenizer=feature_extractor,\n",
    "    compute_metrics=compute_metrics,\n",
    "    data_collator=collate_fn,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\ProgramData\\Anaconda3\\lib\\site-packages\\transformers\\optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 8139\n",
      "  Num Epochs = 3\n",
      "  Instantaneous batch size per device = 32\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 128\n",
      "  Gradient Accumulation steps = 4\n",
      "  Total optimization steps = 189\n",
      "  Number of trainable parameters = 27520892\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f3114ebacaa24974913c149494285eb0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/189 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.6542, 'learning_rate': 2.6315789473684212e-05, 'epoch': 0.16}\n",
      "{'loss': 0.4697, 'learning_rate': 4.970588235294118e-05, 'epoch': 0.31}\n",
      "{'loss': 0.243, 'learning_rate': 4.6764705882352944e-05, 'epoch': 0.47}\n",
      "{'loss': 0.2339, 'learning_rate': 4.382352941176471e-05, 'epoch': 0.63}\n",
      "{'loss': 0.156, 'learning_rate': 4.0882352941176474e-05, 'epoch': 0.78}\n",
      "{'loss': 0.1582, 'learning_rate': 3.794117647058824e-05, 'epoch': 0.94}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 905\n",
      "  Batch size = 32\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c360b62ccfa34d628e6b5805f8d0c280",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/29 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to swin-tiny-patch4-window7-224-finetuned-eurosat\\checkpoint-63\n",
      "Configuration saved in swin-tiny-patch4-window7-224-finetuned-eurosat\\checkpoint-63\\config.json\n",
      "Model weights saved in swin-tiny-patch4-window7-224-finetuned-eurosat\\checkpoint-63\\pytorch_model.bin\n",
      "Image processor saved in swin-tiny-patch4-window7-224-finetuned-eurosat\\checkpoint-63\\preprocessor_config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.04983628913760185, 'eval_accuracy': 0.9767955801104973, 'eval_runtime': 3.8404, 'eval_samples_per_second': 235.65, 'eval_steps_per_second': 7.551, 'epoch': 0.99}\n",
      "{'loss': 0.1641, 'learning_rate': 3.5e-05, 'epoch': 1.11}\n",
      "{'loss': 0.0999, 'learning_rate': 3.205882352941177e-05, 'epoch': 1.27}\n",
      "{'loss': 0.093, 'learning_rate': 2.9117647058823534e-05, 'epoch': 1.42}\n",
      "{'loss': 0.1126, 'learning_rate': 2.6176470588235295e-05, 'epoch': 1.58}\n",
      "{'loss': 0.0947, 'learning_rate': 2.323529411764706e-05, 'epoch': 1.74}\n",
      "{'loss': 0.0962, 'learning_rate': 2.0294117647058825e-05, 'epoch': 1.89}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 905\n",
      "  Batch size = 32\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "df6e550de1194da49dee16e6deaa77bd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/29 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to swin-tiny-patch4-window7-224-finetuned-eurosat\\checkpoint-126\n",
      "Configuration saved in swin-tiny-patch4-window7-224-finetuned-eurosat\\checkpoint-126\\config.json\n",
      "Model weights saved in swin-tiny-patch4-window7-224-finetuned-eurosat\\checkpoint-126\\pytorch_model.bin\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.013440373353660107, 'eval_accuracy': 0.9977900552486187, 'eval_runtime': 3.886, 'eval_samples_per_second': 232.889, 'eval_steps_per_second': 7.463, 'epoch': 1.99}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Image processor saved in swin-tiny-patch4-window7-224-finetuned-eurosat\\checkpoint-126\\preprocessor_config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.1116, 'learning_rate': 1.735294117647059e-05, 'epoch': 2.06}\n",
      "{'loss': 0.0947, 'learning_rate': 1.4411764705882352e-05, 'epoch': 2.22}\n",
      "{'loss': 0.0888, 'learning_rate': 1.1470588235294118e-05, 'epoch': 2.38}\n",
      "{'loss': 0.0801, 'learning_rate': 8.529411764705883e-06, 'epoch': 2.53}\n",
      "{'loss': 0.0752, 'learning_rate': 5.588235294117647e-06, 'epoch': 2.69}\n",
      "{'loss': 0.095, 'learning_rate': 2.647058823529412e-06, 'epoch': 2.85}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 905\n",
      "  Batch size = 32\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "970c81b7bcdb400caac890bbaf7d9713",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/29 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to swin-tiny-patch4-window7-224-finetuned-eurosat\\checkpoint-189\n",
      "Configuration saved in swin-tiny-patch4-window7-224-finetuned-eurosat\\checkpoint-189\\config.json\n",
      "Model weights saved in swin-tiny-patch4-window7-224-finetuned-eurosat\\checkpoint-189\\pytorch_model.bin\n",
      "Image processor saved in swin-tiny-patch4-window7-224-finetuned-eurosat\\checkpoint-189\\preprocessor_config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.012245249934494495, 'eval_accuracy': 0.9977900552486187, 'eval_runtime': 3.8408, 'eval_samples_per_second': 235.628, 'eval_steps_per_second': 7.551, 'epoch': 2.99}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best model from swin-tiny-patch4-window7-224-finetuned-eurosat\\checkpoint-126 (score: 0.9977900552486187).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 261.2386, 'train_samples_per_second': 93.466, 'train_steps_per_second': 0.723, 'train_loss': 0.16926303111686908, 'epoch': 2.99}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "train_results = trainer.train()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token is valid.\n",
      "Your token has been saved in your configured git credential helpers (manager).\n",
      "Your token has been saved to C:\\Users\\Nikodem\\.huggingface\\token\n",
      "Login successful\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to swin-tiny-patch4-window7-224-finetuned-eurosat\n",
      "Configuration saved in swin-tiny-patch4-window7-224-finetuned-eurosat\\config.json\n",
      "Model weights saved in swin-tiny-patch4-window7-224-finetuned-eurosat\\pytorch_model.bin\n",
      "Image processor saved in swin-tiny-patch4-window7-224-finetuned-eurosat\\preprocessor_config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***** train metrics *****\n",
      "  epoch                    =       2.99\n",
      "  train_loss               =     0.1693\n",
      "  train_runtime            = 0:04:21.23\n",
      "  train_samples_per_second =     93.466\n",
      "  train_steps_per_second   =      0.723\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# rest is optional but nice to have\n",
    "trainer.save_model()\n",
    "trainer.log_metrics(\"train\", train_results.metrics)\n",
    "trainer.save_metrics(\"train\", train_results.metrics)\n",
    "trainer.save_state()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 905\n",
      "  Batch size = 32\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6a542e52e7ba44559684de2cbde77090",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/29 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***** eval metrics *****\n",
      "  epoch                   =       2.99\n",
      "  eval_accuracy           =     0.9978\n",
      "  eval_loss               =     0.0134\n",
      "  eval_runtime            = 0:00:04.09\n",
      "  eval_samples_per_second =    221.082\n",
      "  eval_steps_per_second   =      7.084\n"
     ]
    }
   ],
   "source": [
    "\n",
    "metrics = trainer.evaluate()\n",
    "# some nice to haves:\n",
    "trainer.log_metrics(\"eval\", metrics)\n",
    "trainer.save_metrics(\"eval\", metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ad2bdc8ecc057115af97d19610ffacc2b4e99fae6737bb82f5d7fb13d2f2c186"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
